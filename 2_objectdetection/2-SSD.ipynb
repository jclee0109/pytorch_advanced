{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-62208b873035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# パッケージのimport\n",
    "import os.path as osp\n",
    "import random\n",
    "# XMLをファイルやテキストから読み込んだり、加工したり、保存したりするためのライブラリ\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import cv2\n",
    "# pip install opencv-python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    \"\"\"\n",
    "    데이터 경로를 저장한 리스트 작성\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rootpath : str\n",
    "        데이터 경로\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
    "        데이터 경로를 저장한 리스트   \n",
    "    \"\"\"\n",
    "    \n",
    "    # 이미지 파일과 어노테이션 파일의 경로 템플릿 작성\n",
    "    imgpath_template = osp.join(rootpath, \"JPEGImages\", \"%s.jpg\")\n",
    "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
    "    \n",
    "    # 훈련 및 검증 파일 ID(파일 이름) 취득\n",
    "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
    "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
    "    \n",
    "    # 훈련 데이터의 이미지 파일과 어노테이션 파일의 경로 리스트 작성\n",
    "    train_img_list = []\n",
    "    train_anno_list = []\n",
    "    \n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip() # 공백과 줄 바꿈 제거\n",
    "        img_path = (imgpath_template % file_id) # 이미지 경로\n",
    "        anno_path = (annopath_template % file_id) # 어노테이션 경로\n",
    "        train_img_list.append(img_path)\n",
    "        train_anno_list.append(anno_path)\n",
    "        \n",
    "    # 검증 데이터의 이미지 파일과 어노테이션 파일의 경로 리스트 작성\n",
    "    val_img_list = []\n",
    "    val_anno_list = []\n",
    "    \n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip() # 공백과 줄 바꿈 제거\n",
    "        img_path = (imgpath_template % file_id) # 이미지 경로\n",
    "        anno_path = (annopath_template % file_id) # 어노테이션 경로\n",
    "        val_img_list.append(img_path)\n",
    "        val_anno_list.append(anno_path)\n",
    "    \n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
    "\n",
    "print(train_img_list[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.5 xml 형식의 어노테이션 데이터 리스트를 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML 형식의 어노테이션을 리스트 형식으로 변환하는 클래스\n",
    "\n",
    "class Anno_xml2list(object):\n",
    "    \"\"\"\n",
    "    한 이미지의  XML 형식 어노테잇ㄴ 데이터를 이미지 크기로 규격화하여 리스트 형식으로 변환\n",
    "    bbox 크기 맞추는 것이 필요한 거야!!\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    classes : list\n",
    "        VOC 데이터셋의 클래스 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "    \n",
    "    def __call__(self, xml_path, width, height) -> list:\n",
    "        \"\"\"\n",
    "        bbox 의 크기를 이미지의 크기에 맞춰야 하므로, width, height를 받아서 사용\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ret : [[xmin, ymin, xmax, ymax, label_ind], ...]\n",
    "            물체의 어노테이션 데이터를 리스트, 이미지에 존재하는 물체 수만큼의 요소를 가진다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 이미지 내 모든 물체의 어노테이션을 이 리스트에 저장\n",
    "        ret = []\n",
    "        \n",
    "        # xml 파일 로드\n",
    "        xml = ET.parse(xml_path).getroot()\n",
    "        \n",
    "        # 이미지 내 물체(object) 수만큼 반복\n",
    "        for obj in xml.iter('objecdt'):\n",
    "            # 어노테이션에서 탐지가 difficult로 설정된 것은 제외\n",
    "            difficult = int(obj.find('difficult').text)\n",
    "            if difficult == 1:\n",
    "                continue\n",
    "                \n",
    "            # 한 물체의 어노테이션을 저장하는 리스트\n",
    "            bndbox = []\n",
    "            name = obj.find('name').text.lower().strip() # 물체 이름\n",
    "            bbox = obj.find('bndbox') # 바운딩 박스 정보\n",
    "            \n",
    "            # 어노테이션의 xmin, ymin, xmax, ymax를 취득하고 0~1로 규격화\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            \n",
    "            for pt in (pts):\n",
    "                # VOC는 원점이 (1, 1)이므로 1을 빼서 (0,0)으로 만들어준다.\n",
    "                cur_pixel = int(bbox.find(pt).text) - 1\n",
    "                \n",
    "                # 폭, 높이로 규격화\n",
    "                if pt == 'xmin' or pt =='xmax': # x 방향의 경우 폭으로 나눈다.\n",
    "                    cur_pixel /= width\n",
    "                else: # y 방향의 경우 높이로 나눈다.\n",
    "                    cur_pixel /= height\n",
    "                    \n",
    "                bndbox.append(cur_pixel)\n",
    "            \n",
    "            # 어노테이션 클래스명 index를 취득하여 추가\n",
    "            label_idx = self.classes.index(name)\n",
    "            bndbox.append(label_idx)\n",
    "            \n",
    "            # res에 [xmin, ymin, xmax, ymax, label_ind]를 추가\n",
    "            ret += [bndbox]\n",
    "            \n",
    "        return np.array(ret) # [[xmin, ymin, xmax, ymax, label_ind], ...]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인\n",
    "voc_classes = ['aeroaplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "# 화상 로드용으로 OpenCV 사용\n",
    "ind = 1\n",
    "image_file_path = val_img_list[ind]\n",
    "img = cv2.imread(image_file_path) # [높이][너비][색BGR]\n",
    "height, width, channels = img.shape # 화상 크기 취득\n",
    "\n",
    "# 어노테이션을 리스트로 표시\n",
    "transform_anno(val_anno_list[ind], width, height)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.6 화상과 어노테이션의 전처리를 실시하는 DataTransform 클래스 작성\n",
    "학습 시와 추론 시 다르게 작동\n",
    "\n",
    "- 이미지 크기가 데이터마다 다를 수 있어서, 이걸 규격화해주는 거야.\n",
    "- 이미지 크기를 바꿔주면 bbox의 크기도 바뀌어야하기 때문에 이걸 직접 만들어줘야함 torch에 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils 폴더에 있는 ㅇata_augumentation.py 에서 import\n",
    "# 입력 영상의 전처리 클래스\n",
    "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
    "\n",
    "class DataTransform():\n",
    "    \"\"\"\n",
    "    이미지와 어노테이션의 전처리 클래스. 훈련과 추론에서 다르게 작동한다.\n",
    "    이미지 크기를 300 x 300 으로 한다.\n",
    "    학습 시 데이터 확장을 수행한다.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    input_size : int\n",
    "        리사이즈 대상 이미지의 크기\n",
    "    color_mean : (B, G, R)\n",
    "        각 색상 채널의 평균값\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, color_mean):\n",
    "        self.data_transform = {\n",
    "            'train' : Compose([\n",
    "                ConvertFromInts(), # int형을 float형으로 변환\n",
    "                ToAbsoluteCoords(), # 어노테이션 데이터를 절대 좌표값으로 변환\n",
    "                PhotometricDistort(), # 색상을 변환\n",
    "                Expand(color_mean), # 이미지의 캐넙스 확대\n",
    "                RandomSampleCrop(), # 특정 부분 무작위로 추출\n",
    "                RandomMirror(), # 좌우 반전\n",
    "                ToPercentCoords(), # 어노테이션 데이터를 0~1로 정규화\n",
    "                Resize(input_size), # 입력 크기에 맞춰서 리사이즈\n",
    "                SubtractMeans(color_mean) # 색상의 평균을 빼서 정규화\n",
    "            ]),\n",
    "            'val' : Compose([\n",
    "                ConvertFromInts(), # int형을 float형으로 변환\n",
    "                Resize(input_size), # 입력 크기에 맞춰서 리사이즈\n",
    "                SubtractMeans(color_mean) # 색상의 평균을 빼서 정규화\n",
    "            ])\n",
    "        }\n",
    "\n",
    "def __call__(self, img, phase, boxes, labels):\n",
    "    return self.data_transform[phase](img, boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인\n",
    "\n",
    "# 1. 이미지 읽기\n",
    "image_file_path = train_img_list[0]\n",
    "img = cv2.imread(image_file_path) # [높이][폭][색BGR]\n",
    "height, width, shape = img.shape\n",
    "\n",
    "# 2. 어노테이션을 리스트로\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "anno_list = transform_anno(train_anno_list[0], width, height)\n",
    "\n",
    "# 3. 원본 표시\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# 4. 전처리 클래스 작성\n",
    "color_mean = (104, 117, 123) # (BGR) 색상의 평균값\n",
    "input_size = 300 # 이미지 input 사이즈를 300x300으로\n",
    "transform = DataTransform(input_size, color_mean)\n",
    "\n",
    "# 5. Train 이미지 표시\n",
    "phase = 'train'\n",
    "img_transformed, boxes, labels = transform(img, phase, anno_list[:, :4], anno_list[:, 4])\n",
    "plt.imshow(img_transformed)\n",
    "plt.show()\n",
    "\n",
    "# 6. Val 화상 표시\n",
    "phase = 'val'\n",
    "img_transformed, boxes, labels = transform(img, phase, anno_list[:, :4], anno_list[:, 4])\n",
    "plt.imshow(img_transformed)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    VOC2012의 Dataset을 \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    img_list : 리스트\n",
    "        화상 경로를 저장한 리스트\n",
    "    anno_list : 리스트\n",
    "        어노테이션 경로를 저장한 리스트\n",
    "    phase : 'train' or 'test'\n",
    "        학습 또는 훈련 설정\n",
    "    transform : object\n",
    "        전처리 클래스의 인스턴스\n",
    "    transform_anno : object\n",
    "        xml 어노테이션을 리스트로 변환하는 인스턴스\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
    "        self.img_list = img_list\n",
    "        self.anno_list = anno_list\n",
    "        self.phase = phase # train or val\n",
    "        self.transform = transform # 이미지 변형\n",
    "        self.transform_anno = transform_anno # 어노테이션 데이터를 xml에서 리스트로 변경\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        전처리한 이미지의 텐서 형식 데이터와 어노테이션 취득\n",
    "        \"\"\"\n",
    "        im, gt, h, w = self.pull_item(index)\n",
    "        return im, gt\n",
    "    \n",
    "    def pull_item(self, index):\n",
    "        \"\"\"\n",
    "        전처리한 화상의 텐서 형식 데이터, 어노테이션, 화상의 높이, 폭 취득\n",
    "        \"\"\"\n",
    "        # 1. 이미지 읽기\n",
    "        image_file_path = self.img_list[index]\n",
    "        img = cv2.imgread(image_file_path) # [높이][폭][색BGR]\n",
    "        height, width, channels = img.shape\n",
    "        \n",
    "        # 2. xml 형식의 어노테이션 정보를 리스트에 저장\n",
    "        anno_file_path = self.anno_list[index]\n",
    "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
    "        \n",
    "        # 3. 전처리 실시\n",
    "        img, boxes, labels = self.transform(img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
    "        \n",
    "        # 색상 채널의 순서가 BGR이므로 RGB로 순서 변경\n",
    "        # (높이, 폭, 색상 채널)의 순서를 (색상 채널, 높이, 폭)으로 변경\n",
    "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
    "        \n",
    "        # BBox 와 라벨을 세트로 한 np.array를 작성. 변수 이름 gt는 ground truth의 약자\n",
    "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "        \n",
    "        return img, gt, height, width\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인\n",
    "color_mean = (104, 117, 123) # (BGR) 색의 평균값\n",
    "input_size = 300 # 이미지 input 사이즈를 300x300으로 한다.\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase='train', transform=DataTransform(input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase='val', transform=DataTransform(input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "# 데이터 출력 예\n",
    "val_dataset.__getitem__(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e41f695651b7f9eec5d425f4e9e4f81376dd32644cfe4a07b59a775b369dc78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
